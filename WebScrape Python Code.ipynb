{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "fe965e34-0a84-43a3-842e-7dcc1ad77f2d",
      "cell_type": "markdown",
      "source": "# Webscraping & Data Transformation of Book Results ðŸ“šðŸ”Ž",
      "metadata": {}
    },
    {
      "id": "49cadf43-3593-468e-8f39-1316ece7e343",
      "cell_type": "markdown",
      "source": "## Step 1: Installing and importing relevant python libraries ðŸ”Ž",
      "metadata": {}
    },
    {
      "id": "1f7b68be-f884-4ff7-9716-93a61f7960fd",
      "cell_type": "markdown",
      "source": "#### Start by installing the relevant Python library. For this demonstration, I am using \"__beautifulsoup4__\" and \"__mysql-connector-python__\".",
      "metadata": {}
    },
    {
      "id": "298963e8-925c-45ed-816a-1abe43f7b933",
      "cell_type": "code",
      "source": "pip install requests beautifulsoup4\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9c22799f-3d10-4c03-9a40-79f7daf351d6",
      "cell_type": "code",
      "source": "pip install mysql-connector-python\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31b07ca5-a685-42cf-ab5b-60bbda3a73b9",
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\nimport mysql.connector\nimport time\nimport re",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d05f2b0-b800-471f-9871-944065e69203",
      "cell_type": "markdown",
      "source": "*I start by installing BeautifulSoup4 a Python library perfect for scraping data straight off webpages-gives me all the tools to yank tags, text, links etc.*",
      "metadata": {}
    },
    {
      "id": "98b4c0da-e780-43d3-8682-bf52f43e4a21",
      "cell_type": "markdown",
      "source": "## Step 2: Database Connection Setup",
      "metadata": {}
    },
    {
      "id": "db54f680-42a9-4cf2-bdca-399294671e95",
      "cell_type": "code",
      "source": "conn = mysql.connector.connect(\n    host=\"localhost\",\n    user=\"root\",\n    password=\"Lauraandlunaforever1993\",\n    database=\"bookstoscrape\"\n)\n\ncursor = conn.cursor()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f8d94c39-4df3-4093-a1e4-5eb1ac37beb3",
      "cell_type": "markdown",
      "source": "## Step 3: Defining The URL's Required",
      "metadata": {}
    },
    {
      "id": "99e246a9-d0fd-44bd-8a9f-0feb06828a44",
      "cell_type": "code",
      "source": "url = 'https://books.toscrape.com/catalogue/page-{}.html'\n\nBASE_URL = 'https://books.toscrape.com/catalogue/'",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4132b659-0487-411c-8ac5-a00c4afb99c9",
      "cell_type": "markdown",
      "source": "## Step 4: Get Page Content",
      "metadata": {}
    },
    {
      "id": "296297cf-5032-4399-a0ad-a1baa7a63072",
      "cell_type": "markdown",
      "source": "#### I create a \"get_page_content\" variable which fetches the HTML and parses through to BeautifulSoup",
      "metadata": {}
    },
    {
      "id": "05864306-7805-45aa-ba39-e266af9947d8",
      "cell_type": "code",
      "source": "def get_page_content(page_url):\n    response = requests.get(page_url)\n    return BeautifulSoup(response.content, 'html.parser') \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3a50fa9a-0603-4402-891a-52482bca37f1",
      "cell_type": "markdown",
      "source": "## Step 5: Retreaving The Book Details __(\"Instock Availability\" & \"Genre\")__.",
      "metadata": {}
    },
    {
      "id": "19945ab3-9f7a-4f0d-a867-9675f955684f",
      "cell_type": "markdown",
      "source": "*It looks for the p class=instock availability tag on the page. Then it searches the text for the number available. If it then finds the number available, it then converts the data type into an integer. It then looks inside the breadcrumb navigation link class=breadcrumb It takes the genre type and extracts the text.*\n\n*With the return values, if it isn't able to identify a number from how many are available it will then return \"0\" as the default. If the Genre isn't extracted it will return none if it's not found.*",
      "metadata": {}
    },
    {
      "id": "61909135-cd2f-4afc-8f75-9488c7642306",
      "cell_type": "code",
      "source": "def get_book_details(detail_url):\n    detail_soup = get_page_content(detail_url)\n\n    availability_tag = detail_soup.find('p', class_='instock availability')\n    number_available = 0\n    if availability_tag:\n        match = re.search(r'\\((\\d+) available\\)', availability_tag.get_text())\n        if match:\n            number_available = int(match.group(1))\n\n    genre = None\n    breadcrumb = detail_soup.find('ul', class_='breadcrumb')\n    if breadcrumb:\n        items = breadcrumb.find_all('li')\n        if len(items) >= 3:\n            genre_tag = items[-2].find('a')\n            if genre_tag:\n                genre = genre_tag.get_text(strip=True)\n\n    return number_available, genre\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4b6622f0-951b-4174-b4f8-7968ad6d308e",
      "cell_type": "markdown",
      "source": "## Step 6: Getting The Book Details __(\"Title, \"Price\", \"Rating\", \"Availability\")__.",
      "metadata": {}
    },
    {
      "id": "39455d28-5061-47ad-a71f-a58fd89905c4",
      "cell_type": "markdown",
      "source": "*This function \"scrape_book_details\" extracts all the important information about a book from the HTML elements. It starts by pulling the title, cost, star rating. The \"rating_mapping\" is a dictionary which assigns the text to it's corresponding numerical values. The Rating is then assigned ot the \"rating_mapping\" with \"(Rating_text, 0.0)\" where any text without any values to be converted to 0. The \"re.seach\" function is used to find the numeric value in the stock availability test. Using the regex pattern it will extract the number from that. Then the interger is set for the data type under \"Number_Available\" and if match extracts the number, and the else 0 if it's unable to convert to an interger.*  \n\n*Next a link to the bookâ€™s detail page, uses get_book_details to fetch the genre from that pages breadcrumb, and finally returns everything as a dictionary containing the title, cost, rating, stock status, number available, and genre.",
      "metadata": {}
    },
    {
      "id": "0bf207ad-debc-4842-872e-a467781e1e73",
      "cell_type": "code",
      "source": "def scrape_book_details(book_container):\n    Title = book_container.find('h3').find('a')['title']\n    Cost = book_container.find('p', class_='price_color').text.strip('Â£').replace('Ã‚', '')\n    Rating_text = book_container.find('p', class_='star-rating')['class'][1]\n    Stock_Availability = book_container.find('p', class_='instock availability').text.strip()\n    \n    \n    match = re.search(r'\\((\\d+) available\\)', Stock_Availability)\n    Number_Available = int(match.group(1)) if match else 0\n\n    rating_mapping = {\n        \"One\": 1.0,\n        \"Two\": 2.0,\n        \"Three\": 3.0,\n        \"Four\": 4.0,\n        \"Five\": 5.0\n    }\n    Rating = rating_mapping.get(Rating_text, 0.0)\n\n    detail_href = book_container.find('h3').find('a')['href']\n    detail_url = BASE_URL + detail_href.replace('../../', '')\n\n    number_available, genre = get_book_details(detail_url)\n\n    return {\n        \"Title\": Title,\n        \"Cost\": Cost,\n        \"Rating\": Rating,\n        \"Stock_Availability\": Stock_Availability,\n        \"Number_Available\": number_available,\n        \"Genre\": genre\n    }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "97408359-3181-4a46-b767-432e4c2d7098",
      "cell_type": "markdown",
      "source": "## Step 7: Scraping Each Page.",
      "metadata": {}
    },
    {
      "id": "2c3e7660-c929-4e64-82c2-b78feb72cd9b",
      "cell_type": "markdown",
      "source": "*I create a \"scrape_books_from_page\" variable which parses the page_url to BeautifulSoup. The \"soup.find_all\" method which finds the ('article', class_='product_pod') on that page under \" book_containers\".*   \n\n*This is then placed into a list and a for loop is applied to the container in \"book_containers\" and is then added to the \"scrape_book_details\" container where the other data sets are stored.*",
      "metadata": {}
    },
    {
      "id": "d008e7cb-71e2-4903-9eb2-d20dd5826fd0",
      "cell_type": "code",
      "source": "def scrape_books_from_page(page_url):\n    soup = get_page_content(page_url)\n    book_containers = soup.find_all('article', class_='product_pod')\n    books = []\n    for container in book_containers:\n        books.append(scrape_book_details(container))\n    return books\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2b03867c-0c23-453c-89a8-098778799da9",
      "cell_type": "markdown",
      "source": "## Step 8: Test The Result Of The First Page.",
      "metadata": {}
    },
    {
      "id": "2b274fc1-55b1-4bd3-a77a-deff1f0f5d56",
      "cell_type": "markdown",
      "source": "*A for loop is created to scan through \"books_on_first_page\" to scrape through the first page and pulls the books details of the title, cost, rating, stock availability, number available, and genre. It then prints out those details or breaks the loop with the \"is not None\" and handles it to the else print, to state \"Warning: Book details could not be scraped\".*",
      "metadata": {}
    },
    {
      "id": "c03a2e44-db03-4513-9377-3f3d27c56e24",
      "cell_type": "code",
      "source": "page_url = url.format(1)\nbooks_on_first_page = scrape_books_from_page(page_url)\n\nfor book in books_on_first_page:\n    if book is not None:\n        print(f\"Title: {book['Title']}, Cost: Â£{book['Cost']}, Rating: {book['Rating']} \"\n              f\"Stock_Availability: {book['Stock_Availability']} \"\n              f\"Number_Available: {book['Number_Available']} Genre: {book['Genre']}\")\n    else:\n        print(\"Warning: Book details could not be scraped.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ebebb256-49be-4046-92b0-c2214acc9c87",
      "cell_type": "markdown",
      "source": "## Step 9: Scraping All Pages.",
      "metadata": {}
    },
    {
      "id": "d4a672eb-a47e-4652-a3c5-b565241fb027",
      "cell_type": "markdown",
      "source": "*A while loop is created to itterate through each page. The page number is first defined as 1 \"page_number = 1\" and is called to the url to be formatted.*  \n\n*The \"scrape_books_from_page\" then contains the page number with the list of the \"scrape_book_details\" and brings together with \"full_book_and_containers_on_page\".*\n\n*After that the if not statement is applied and the break to stop the loop if there a no longer any pages to go through. This is all stored under a list under \"all_books\" = [] under the \"all_books.extend\" method.*\n\n*To find the results, I then print and call the page number and the number of books within each given page, the \"page_number += 1\" allows for page-by page increase to loop through after each result is printed until the if not method.*\n\n*A timer is applied for two seconds to allow for a delay for each page before continuing the scrape to prevent any overloading with too many requests.*",
      "metadata": {}
    },
    {
      "id": "abb41f93-5800-44c4-9e41-2adcf13f5803",
      "cell_type": "code",
      "source": "all_books = []\npage_number = 1\n\nwhile True:\n    page_url = url.format(page_number)\n    \n    full_book_and_containers_on_page = scrape_books_from_page(page_url)\n\n    if not full_book_and_containers_on_page:\n        break\n    \n    all_books.extend(full_book_and_containers_on_page)\n\n    print(f\"Page {page_number}: Found {len(full_book_and_containers_on_page)} books\")\n    \n    page_number += 1\n    time.sleep(2)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2eb978cf-b279-464d-a7bc-7a4dfcbe8065",
      "cell_type": "markdown",
      "source": "## Step 10: Saving To The MYSQL Database.",
      "metadata": {}
    },
    {
      "id": "22f4d5c3-947c-4bc1-b6db-864f4cadda0a",
      "cell_type": "markdown",
      "source": "#### Then a for loop is used in \"all_books\" to save to the database the \"Title\", \"Cost\", \"Rating\", \"Stock_Availability\", \"Genre\" and \"Number_Available\".",
      "metadata": {}
    },
    {
      "id": "dea82ad1-1390-4d39-a7af-a5fa969228b6",
      "cell_type": "markdown",
      "source": "*This code stores all the scraped books into the database. The function \"savetoDB\" inserts each bookâ€™s details title, cost, rating, stock availability, genre, and number available into the bookdata table. To ensure there were no duplicate values the \"ON DUPLICATE KEY UPDATE\" is used to prevent this from happening. The for loop calls every book in \"all_books\" and saves any missing genres as \"None\". The script carries out the changes to the database and closes the cursor and the connection.*",
      "metadata": {}
    },
    {
      "id": "893d478b-6e2d-44d6-a82b-459df71f5175",
      "cell_type": "code",
      "source": "def savetoDB(title, cost, rating, stock, genre, number):\n    cursor.execute(\"\"\"\n        INSERT INTO bookdata\n        (Title, Cost, Rating, Stock_Availability, Genre, Number_Available)\n        VALUES (%s, %s, %s, %s, %s, %s)\n        ON DUPLICATE KEY UPDATE\n            Cost = VALUES(Cost),\n            Rating = VALUES(Rating),\n            Stock_Availability = VALUES(Stock_Availability),\n            Genre = VALUES(Genre),\n            Number_Available = VALUES(Number_Available)\n    \"\"\", (title, cost, rating, stock, genre, number))\n\nfor book in all_books:\n    if book is not None:\n        savetoDB(\n            book[\"Title\"],\n            book[\"Cost\"],\n            book[\"Rating\"],\n            book[\"Stock_Availability\"],\n            book[\"Genre\"] if book[\"Genre\"] else None,\n            book[\"Number_Available\"]\n        )\n\nconn.commit()\ncursor.close()\nconn.close()\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f1148c0e-376f-4ce0-bf38-9f6bb1bc7ef8",
      "cell_type": "markdown",
      "source": "__Here is the mark down file for the MYSQL table results:__    \nhttps://github.com/Drook93/Webscrape-Books/blob/master/SQL_Results_Books_Scraped.md\n    ",
      "metadata": {}
    },
    {
      "id": "c6dacc05-43f8-42c0-93ea-5553694d0bc6",
      "cell_type": "markdown",
      "source": "__Here is the PowerPoint presentation for the data analysis project:__   \n\nhttps://github.com/Drook93/Webscrape-Books/blob/master/Website%20Scraping%20Project.pdf\n\n\n",
      "metadata": {}
    },
    {
      "id": "0ce2eb24-33c4-48a6-a2bb-8fd1892fd3ae",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8010f9b7-d1cd-4747-975f-2e6d6e55f513",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}